{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Feature Reduction using general genetic algorithm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"chart_perspective.jpg\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import neccessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import isnan\n",
    "import numpy as np\n",
    "import hazm\n",
    "import itertools\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Datasets/final_ds_500-500+.txt\",delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tockenize = pd.read_csv(\"Datasets/final_ds_500-500+.txt\",delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_sample = 0\n",
    "for i_sample in range(data.shape[0]):\n",
    "    try:\n",
    "        word = []\n",
    "        Stem = hazm.Stemmer()\n",
    "        temp = hazm.sent_tokenize(data_tockenize['BODY'][i_sample])\n",
    "        for i_sentenses in temp :\n",
    "            word.append(hazm.word_tokenize(i_sentenses))\n",
    "        data_tockenize['BODY'][i_sample] = word\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        word = []\n",
    "        Stem = hazm.Stemmer()\n",
    "        temp = hazm.sent_tokenize(data_tockenize['TITLE'][i_sample])\n",
    "        for i_sentenses in temp :\n",
    "            word.append(hazm.word_tokenize(i_sentenses))\n",
    "        data_tockenize['TITLE'][i_sample] = word\n",
    "    except:\n",
    "        pass\n",
    "    if(data['Sentiment'][i_sample] == 'مثبت'):\n",
    "        data_tockenize['Sentiment'][i_sample] = 'Pos'\n",
    "    elif(data['Sentiment'][i_sample] == 'منفی'):\n",
    "        data_tockenize['Sentiment'][i_sample] = 'Neg'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to numpyarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_temp = np.asanyarray(data_tockenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delet nan value from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = []\n",
    "for i_sample in range(data_temp.shape[0]):\n",
    "    if(type(data_temp[i_sample,2]) != type([])):\n",
    "        if(isnan(data_temp[i_sample,2])):\n",
    "            del_list.append(i_sample)\n",
    "data_temp = np.delete(data_temp,del_list,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatening data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_sample in range(data_temp.shape[0]):\n",
    "    while(len(data_temp[i_sample,2]) > 1):\n",
    "        data_temp[i_sample,2][0] += data_temp[i_sample,2][1]\n",
    "        del data_temp[i_sample,2][1]\n",
    "        #tmp[0] = np.ndarray.tolist(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = np.zeros((0,2),dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_temp.shape[0]):\n",
    "    words = deepcopy(data_temp[i,1][0])\n",
    "    words += deepcopy(data_temp[i,2][0])\n",
    "    tmp = np.zeros((1,2),dtype='object')\n",
    "    tmp[0,0] = data_temp[i,0]\n",
    "    tmp[0,1] = words\n",
    "    new_data = np.vstack((new_data,tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deleting non relative character like : + - * /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 'ا'\n",
    "b = 'ی'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(new_data.shape[0]):\n",
    "    #for each row\n",
    "    for ii in range(len(new_data[i,1])):\n",
    "        #for each word\n",
    "        del_list = []\n",
    "        for k in range(len(new_data[i,1][ii])):\n",
    "            # for each character\n",
    "            if(not a <= new_data[i,1][ii][k] <= b):\n",
    "                del_list.append(k)\n",
    "        for k in range(len(del_list)):\n",
    "            # for each character\n",
    "            new_data[i,1][ii] = new_data[i,1][ii].replace(new_data[i,1][ii][del_list[k]],';')\n",
    "        new_data[i,1][ii] = new_data[i,1][ii].replace(';','')\n",
    "        \n",
    "    while '' in new_data[i,1] :\n",
    "        new_data[i,1].remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not that data has been cleaned we need to create word set from cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wordset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_sample in range(new_data.shape[0]):\n",
    "    #for each row or news\n",
    "    for i_word in range(len(new_data[i_sample,1])):\n",
    "        #for each word in each news\n",
    "        Wordset.add(new_data[i_sample,1][i_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = np.zeros((0,2),dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating TF vector for each news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_sample in range(new_data.shape[0]):\n",
    "    tmp = np.zeros((1,2),dtype='object')\n",
    "    tmp[0,0] = new_data[i_sample,0]\n",
    "    template = dict.fromkeys(Wordset,0)\n",
    "    for i_word in new_data[i_sample,1]:\n",
    "        template[i_word] += 1\n",
    "    tmp[0,1] = template\n",
    "    dict_data = np.vstack((dict_data,tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf = dict.fromkeys(Wordset,0)\n",
    "for i_sample in range(new_data.shape[0]):\n",
    "    tmp = dict.fromkeys(Wordset,0)\n",
    "    for i_word in new_data[i_sample,1]:\n",
    "        tmp[i_word] = 1\n",
    "    idf = {k: idf.get(k, 0) + tmp.get(k, 0) for k in set(idf) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF for each word in each news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_data = deepcopy(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_sample in range(dict_data.shape[0]):\n",
    "    for word in dict_data[i_sample,1]:\n",
    "        if idf[word] < 800:\n",
    "            idf_data[i_sample,1][word] = dict_data[i_sample,1][word] * ( \\\n",
    "                                                                        np.log(dict_data.shape[0]) - \\\n",
    "                                                                        np.log(idf[word]))\n",
    "        else :\n",
    "            idf_data[i_sample,1][word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_sample in range(dict_data.shape[0]):\n",
    "    idf_data[i_sample,1].update({'class': idf_data[i_sample,0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating featured data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=list(idf_data[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i_sample in range(dict_data.shape[0]):\n",
    "    tmp = pd.DataFrame([idf_data[i_sample,1]],columns=list(idf_data[0,1]))\n",
    "    dataset = dataset.append(tmp)\n",
    "    del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('featured_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame.from_csv('featured_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_data = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average result is :  0.811623246493\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    train,test = train_test_split(tmp_data,test_size = 0.5)\n",
    "\n",
    "    model = SVC()\n",
    "\n",
    "    model.fit(train[:,0:train.shape[1]-1],train[:,train.shape[1]-1])\n",
    "\n",
    "    results.append(float(sum(model.predict(test[:,0:test.shape[1]-1]) == test[:,test.shape[1]-1])) / test.shape[0])\n",
    "print(\"Average result is : \",np.average(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8136272545090181,\n",
       " 0.8016032064128257,\n",
       " 0.8316633266533067,\n",
       " 0.8316633266533067,\n",
       " 0.8156312625250501,\n",
       " 0.8016032064128257,\n",
       " 0.8216432865731463,\n",
       " 0.8136272545090181,\n",
       " 0.8416833667334669,\n",
       " 0.7434869739478958]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
